{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"FastText is a word representation and classification tool developed by Facebook's AI Research (FAIR) lab. It extends the Word2Vec model by representing each word as a bag of character n-grams, which helps capture subword information and improve the handling of rare words.\n\nHere's a simple implementation of FastText from scratch in Python using NumPy. This example will focus on creating word vectors using character n-grams and training a basic model to understand the concept.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nfrom collections import defaultdict\nfrom sklearn.preprocessing import normalize\n\nclass FastText:\n    def __init__(self, vocab_size, embedding_dim, n_gram_size=3, learning_rate=0.01, epochs=10):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.n_gram_size = n_gram_size\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.word_embeddings = np.random.uniform(-0.1, 0.1, (vocab_size, embedding_dim))\n        self.context_embeddings = np.random.uniform(-0.1, 0.1, (vocab_size, embedding_dim))\n        self.vocab = {}\n        self.rev_vocab = {}\n\n    def build_vocab(self, sentences):\n        word_count = defaultdict(int)\n        for sentence in sentences:\n            words = sentence.split()\n            for word in words:\n                word_count[word] += 1\n        self.vocab = {word: idx for idx, (word, _) in enumerate(word_count.items())}\n        self.rev_vocab = {idx: word for word, idx in self.vocab.items()}\n    \n    def get_ngrams(self, word):\n        ngrams = set()\n        word = '<' * (self.n_gram_size - 1) + word + '>' * (self.n_gram_size - 1)\n        for i in range(len(word) - self.n_gram_size + 1):\n            ngrams.add(word[i:i + self.n_gram_size])\n        return ngrams\n\n    def train(self, sentences):\n        for epoch in range(self.epochs):\n            loss = 0\n            for sentence in sentences:\n                words = sentence.split()\n                for i, word in enumerate(words):\n                    if word not in self.vocab:\n                        continue\n                    word_idx = self.vocab[word]\n                    # Collect n-grams for the target word\n                    target_ngrams = self.get_ngrams(word)\n                    # Update context vectors\n                    for j in range(max(0, i - 1), min(len(words), i + 2)):\n                        if i != j and words[j] in self.vocab:\n                            context_idx = self.vocab[words[j]]\n                            # Update embeddings using a simple SGD approach\n                            prediction = self.predict(word_idx, context_idx)\n                            error = prediction - 1 if j == i + 1 else prediction\n                            loss += error**2\n                            self.word_embeddings[word_idx] -= self.learning_rate * error * self.context_embeddings[context_idx]\n                            self.context_embeddings[context_idx] -= self.learning_rate * error * self.word_embeddings[word_idx]\n            print(f'Epoch {epoch + 1}/{self.epochs}, Loss: {loss}')\n\n    def predict(self, word_idx, context_idx):\n        # Calculate the dot product of the word and context embeddings\n        return np.dot(self.word_embeddings[word_idx], self.context_embeddings[context_idx])\n\n    def get_word_vector(self, word):\n        if word in self.vocab:\n            return self.word_embeddings[self.vocab[word]]\n        else:\n            raise ValueError(f\"Word '{word}' not found in vocabulary\")\n\n    def get_embedding_matrix(self):\n        return normalize(self.word_embeddings, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T09:18:13.242580Z","iopub.execute_input":"2024-07-21T09:18:13.243196Z","iopub.status.idle":"2024-07-21T09:18:13.266124Z","shell.execute_reply.started":"2024-07-21T09:18:13.243154Z","shell.execute_reply":"2024-07-21T09:18:13.264965Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Example usage\nsentences = [\n    \"fast text is a library for efficient text classification\",\n    \"word embeddings are useful for NLP tasks\",\n    \"fasttext models can handle out-of-vocabulary words\"\n]\n\nfasttext_model = FastText(vocab_size=100, embedding_dim=50)\nfasttext_model.build_vocab(sentences)\nfasttext_model.train(sentences)\n\n# Get the vector for a word\nvector = fasttext_model.get_word_vector(\"fast\")\nprint(f\"Vector for 'fast': {vector}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T09:18:16.126476Z","iopub.execute_input":"2024-07-21T09:18:16.126846Z","iopub.status.idle":"2024-07-21T09:18:16.142713Z","shell.execute_reply.started":"2024-07-21T09:18:16.126819Z","shell.execute_reply":"2024-07-21T09:18:16.141594Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss: 19.119878852379344\nEpoch 2/10, Loss: 18.99453025838787\nEpoch 3/10, Loss: 18.870092708139556\nEpoch 4/10, Loss: 18.746487508551116\nEpoch 5/10, Loss: 18.623638717743134\nEpoch 6/10, Loss: 18.501473031096197\nEpoch 7/10, Loss: 18.3799196759705\nEpoch 8/10, Loss: 18.258910314547194\nEpoch 9/10, Loss: 18.13837895429284\nEpoch 10/10, Loss: 18.018261865586297\nVector for 'fast': [-0.05548203 -0.03690686 -0.09314208 -0.03563283  0.06050951  0.020029\n -0.05077991  0.08787123 -0.01413946  0.0086497  -0.03687215  0.05203214\n  0.01835446 -0.06844073  0.09027416 -0.02818438  0.10348338 -0.07489015\n  0.07761034 -0.04408902  0.0767948  -0.06356193 -0.00514316 -0.04496691\n  0.05477788  0.00410989 -0.05043605 -0.09628502  0.00932644 -0.04340611\n  0.01002255  0.08901038  0.08351978  0.00404784  0.05514393  0.0440209\n -0.01103182  0.05813477  0.00603558 -0.09994054 -0.04833565 -0.03460841\n -0.05296038  0.08717721 -0.05086532 -0.10222166  0.08960338 -0.00234772\n  0.05215666 -0.07103336]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Explanation\n\n1. **Initialization**:\n   - `vocab_size`: Size of the vocabulary.\n   - `embedding_dim`: Dimension of the word embeddings.\n   - `n_gram_size`: Size of character n-grams.\n   - `learning_rate`: Learning rate for updating embeddings.\n   - `epochs`: Number of training epochs.\n\n2. **Building Vocabulary**:\n   - `build_vocab()`: Constructs the vocabulary from the sentences and creates reverse mapping.\n\n3. **Generating N-grams**:\n   - `get_ngrams()`: Generates character n-grams for a given word. The word is padded with `<` and `>` symbols to capture edge cases.\n\n4. **Training**:\n   - `train()`: Updates word and context embeddings using a simple SGD approach. The loss is computed as the squared error between the predicted and actual values.\n\n5. **Prediction**:\n   - `predict()`: Calculates the dot product between the target word and context embeddings.\n\n6. **Getting Word Vectors**:\n   - `get_word_vector()`: Retrieves the embedding for a specific word.\n\n7. **Normalization**:\n   - `get_embedding_matrix()`: Returns the normalized embedding matrix.\n\nThis code provides a simplified version of FastText. Real-world implementations are more complex, involving negative sampling, hierarchical softmax, and optimized training methods for efficiency.","metadata":{}}]}