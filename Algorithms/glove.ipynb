{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### What is GloVe?\n\nGloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm developed by researchers at Stanford University, including Jeffrey Pennington, Richard Socher, and Christopher Manning. It is used to generate dense vector representations of words, capturing semantic relationships between them. Unlike traditional one-hot encoding, which creates sparse vectors, GloVe produces low-dimensional, continuous vectors that encode meaningful information about words and their contexts.\n\n### Key Concepts of GloVe\n\n1. **Co-occurrence Matrix**: GloVe starts by constructing a co-occurrence matrix from a large corpus of text. This matrix counts how often words appear together within a given context window. Each entry in the matrix represents the frequency of co-occurrence between two words.\n\n2. **Weighted Least Squares**: The core idea of GloVe is to factorize this co-occurrence matrix using a weighted least squares objective. This objective ensures that words frequently appearing together in similar contexts have similar vector representations.\n\n3. **Weighting Function**: To balance the influence of frequent and rare co-occurrences, GloVe employs a weighting function that gives less importance to very frequent co-occurrences, preventing them from dominating the optimization process.\n\n4. **Training Objective**: The training objective minimizes the difference between the dot product of word vectors and the logarithm of their co-occurrence count. This objective ensures that similar words (based on their context) have similar vector representations.\n\n### GloVe Training Objective\n\nThe objective function of GloVe is designed to capture the statistical information encoded in the co-occurrence matrix. For two words $i$ and $j$ with vector representations $w_i$ and $w_j$, the objective function can be expressed as:\n\n$$ \nJ = \\sum_{i,j=1}^{V} f(X_{ij}) \\left( w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\right)^2 \n$$\n\nwhere:\n- $ V $ is the size of the vocabulary.\n- $ X_{ij} $ is the co-occurrence count of words $i$ and $j$.\n- $ f(X_{ij}) $ is the weighting function.\n- $ w_i $ and $ \\tilde{w}_j $ are the word vectors for words $i$ and $j$, respectively.\n- $ b_i $ and $ \\tilde{b}_j $ are bias terms.\n\nThe weighting function $ f(X_{ij}) $ is defined as:\n\n$$ \nf(X_{ij}) = \\left( \\frac{X_{ij}}{X_{\\text{max}}} \\right)^\\alpha \\quad \\text{if } X_{ij} < X_{\\text{max}}, \\text{otherwise } f(X_{ij}) = 1 \n$$\n\nwhere:\n- $ X_{\\text{max}} $ and $ \\alpha $ are hyperparameters.\n\n\n### Advantages of GloVe\n\n1. **Efficient Training**: GloVe is trained on a global co-occurrence matrix, which allows it to capture long-range dependencies and semantic relationships effectively.\n2. **Meaningful Vectors**: The resulting word vectors capture semantic relationships, such as word analogies (e.g., \"king\" - \"man\" + \"woman\" â‰ˆ \"queen\").\n3. **Flexibility**: GloVe can be applied to various tasks, including word similarity, named entity recognition, and machine translation.\n\n### Applications\n\n- **Natural Language Processing (NLP)**: GloVe vectors are used as input features for various NLP tasks, such as sentiment analysis, named entity recognition, and question answering.\n- **Information Retrieval**: Word vectors help in improving search engine algorithms by providing better semantic matching of queries and documents.\n- **Machine Translation**: GloVe embeddings are used to improve the performance of machine translation models by capturing semantic similarities between words in different languages.\n\n### Conclusion\n\nGloVe is a powerful technique for generating word embeddings that capture semantic relationships between words. Its ability to learn meaningful representations from large corpora makes it a valuable tool in various NLP and machine learning applications.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport itertools\nimport string\nfrom sklearn.preprocessing import normalize\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-07-21T09:06:55.436407Z","iopub.execute_input":"2024-07-21T09:06:55.437229Z","iopub.status.idle":"2024-07-21T09:06:55.442496Z","shell.execute_reply.started":"2024-07-21T09:06:55.437188Z","shell.execute_reply":"2024-07-21T09:06:55.441274Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def preprocess(text):\n    \"\"\"\n    Preprocess the text by removing punctuation, converting to lowercase, and splitting into words.\n    \n    Args:\n    text (str): Input text string.\n    \n    Returns:\n    list: List of words (tokens).\n    \"\"\"\n    # Remove punctuation and convert to lowercase\n    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n    # Split into words (tokens)\n    tokens = text.split()\n    return tokens\n\n# Example text corpus\ncorpus = [\"I love NLP\", \"NLP is a fascinating field\", \"Natural language processing with GloVe\"]\n\n# Preprocess the corpus\ntokens = list(itertools.chain(*[preprocess(sentence) for sentence in corpus]))\n\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T09:00:48.265342Z","iopub.execute_input":"2024-07-21T09:00:48.266192Z","iopub.status.idle":"2024-07-21T09:00:48.274328Z","shell.execute_reply.started":"2024-07-21T09:00:48.266158Z","shell.execute_reply":"2024-07-21T09:00:48.273119Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"['i', 'love', 'nlp', 'nlp', 'is', 'a', 'fascinating', 'field', 'natural', 'language', 'processing', 'with', 'glove']\n","output_type":"stream"}]},{"cell_type":"code","source":"def build_vocab(tokens):\n    \"\"\"\n    Build vocabulary from tokens and create a word-to-index mapping.\n    \n    Args:\n    tokens (list): List of words (tokens).\n    \n    Returns:\n    dict: Word-to-index mapping.\n    \"\"\"\n    vocab = Counter(tokens)\n    word_to_index = {word: i for i, word in enumerate(vocab)}\n    return word_to_index\n\ndef build_cooccurrence_matrix(tokens, word_to_index, window_size=2):\n    \"\"\"\n    Build the co-occurrence matrix from tokens using a specified window size.\n    \n    Args:\n    tokens (list): List of words (tokens).\n    word_to_index (dict): Word-to-index mapping.\n    window_size (int): Context window size.\n    \n    Returns:\n    np.ndarray: Co-occurrence matrix.\n    \"\"\"\n    vocab_size = len(word_to_index)\n    cooccurrence_matrix = np.zeros((vocab_size, vocab_size))\n    \n    for i, word in enumerate(tokens):\n        word_index = word_to_index[word]\n        context_start = max(0, i - window_size)\n        context_end = min(len(tokens), i + window_size + 1)\n        \n        for j in range(context_start, context_end):\n            if i != j:\n                context_word = tokens[j]\n                context_word_index = word_to_index[context_word]\n                cooccurrence_matrix[word_index, context_word_index] += 1\n    \n    return cooccurrence_matrix\n\n# Build vocabulary and co-occurrence matrix\nword_to_index = build_vocab(tokens)\ncooccurrence_matrix = build_cooccurrence_matrix(tokens, word_to_index)\n\nprint(cooccurrence_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T09:00:56.458264Z","iopub.execute_input":"2024-07-21T09:00:56.459290Z","iopub.status.idle":"2024-07-21T09:00:56.473657Z","shell.execute_reply.started":"2024-07-21T09:00:56.459247Z","shell.execute_reply":"2024-07-21T09:00:56.472468Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[[0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [1. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [1. 2. 2. 2. 1. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 2. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n [0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0.]\n [0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0.]\n [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]]\n","output_type":"stream"}]},{"cell_type":"code","source":"class GloVe:\n    def __init__(self, vocab_size, embedding_dim=50, x_max=100, alpha=0.75):\n        \"\"\"\n        Initialize the GloVe model with the specified parameters.\n        \n        Args:\n        vocab_size (int): Size of the vocabulary.\n        embedding_dim (int): Dimensionality of the word vectors.\n        x_max (int): Maximum value for the weighting function.\n        alpha (float): Exponent for the weighting function.\n        \"\"\"\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.x_max = x_max\n        self.alpha = alpha\n        self.W = np.random.rand(vocab_size, embedding_dim)\n        self.W_tilde = np.random.rand(vocab_size, embedding_dim)\n        self.b = np.random.rand(vocab_size)\n        self.b_tilde = np.random.rand(vocab_size)\n        self.gradsq_W = np.ones((vocab_size, embedding_dim))\n        self.gradsq_W_tilde = np.ones((vocab_size, embedding_dim))\n        self.gradsq_b = np.ones(vocab_size)\n        self.gradsq_b_tilde = np.ones(vocab_size)\n    \n    def weighting_function(self, x):\n        \"\"\"\n        Weighting function for the GloVe model.\n        \n        Args:\n        x (float): Co-occurrence value.\n        \n        Returns:\n        float: Weight.\n        \"\"\"\n        if x < self.x_max:\n            return (x / self.x_max) ** self.alpha\n        return 1.0\n    \n    def train(self, cooccurrence_matrix, epochs=100, learning_rate=0.05):\n        \"\"\"\n        Train the GloVe model using the specified co-occurrence matrix.\n        \n        Args:\n        cooccurrence_matrix (np.ndarray): Co-occurrence matrix.\n        epochs (int): Number of training epochs.\n        learning_rate (float): Learning rate for gradient updates.\n        \"\"\"\n        for epoch in range(epochs):\n            total_cost = 0\n            for i in range(self.vocab_size):\n                for j in range(self.vocab_size):\n                    if cooccurrence_matrix[i, j] == 0:\n                        continue\n                    X_ij = cooccurrence_matrix[i, j]\n                    weight = self.weighting_function(X_ij)\n                    cost = weight * (np.dot(self.W[i], self.W_tilde[j]) + self.b[i] + self.b_tilde[j] - np.log(X_ij)) ** 2\n                    total_cost += cost\n                    \n                    # Compute gradients\n                    grad_common = weight * (np.dot(self.W[i], self.W_tilde[j]) + self.b[i] + self.b_tilde[j] - np.log(X_ij))\n                    grad_W = grad_common * self.W_tilde[j]\n                    grad_W_tilde = grad_common * self.W[i]\n                    grad_b = grad_common\n                    grad_b_tilde = grad_common\n                    \n                    # Update parameters\n                    self.W[i] -= learning_rate * grad_W / np.sqrt(self.gradsq_W[i])\n                    self.W_tilde[j] -= learning_rate * grad_W_tilde / np.sqrt(self.gradsq_W_tilde[j])\n                    self.b[i] -= learning_rate * grad_b / np.sqrt(self.gradsq_b[i])\n                    self.b_tilde[j] -= learning_rate * grad_b_tilde / np.sqrt(self.gradsq_b_tilde[j])\n                    \n                    # Update squared gradients\n                    self.gradsq_W[i] += grad_W ** 2\n                    self.gradsq_W_tilde[j] += grad_W_tilde ** 2\n                    self.gradsq_b[i] += grad_b ** 2\n                    self.gradsq_b_tilde[j] += grad_b_tilde ** 2\n            \n            if epoch % 10 == 0:\n                print(f'Epoch: {epoch}, Cost: {total_cost}')\n    \n    def get_word_vector(self, word):\n        \"\"\"\n        Get the word vector for the specified word.\n        \n        Args:\n        word (str): Input word.\n        \n        Returns:\n        np.ndarray: Word vector.\n        \"\"\"\n        if word in word_to_index:\n            word_index = word_to_index[word]\n            return self.W[word_index]\n        return None\n\n# Initialize and train the GloVe model\nglove = GloVe(vocab_size=len(word_to_index), embedding_dim=50)\nglove.train(cooccurrence_matrix, epochs=100)\n\n# Get the word vector for 'nlp'\nword_vector = glove.get_word_vector('nlp')\nprint(word_vector)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T09:01:05.705027Z","iopub.execute_input":"2024-07-21T09:01:05.705447Z","iopub.status.idle":"2024-07-21T09:01:05.867867Z","shell.execute_reply.started":"2024-07-21T09:01:05.705412Z","shell.execute_reply":"2024-07-21T09:01:05.866477Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch: 0, Cost: 212.64408285952362\nEpoch: 10, Cost: 33.981420342290654\nEpoch: 20, Cost: 11.90269477883924\nEpoch: 30, Cost: 5.175079400732769\nEpoch: 40, Cost: 2.5389622248442816\nEpoch: 50, Cost: 1.3609105311592509\nEpoch: 60, Cost: 0.7840870656955667\nEpoch: 70, Cost: 0.47969041286440744\nEpoch: 80, Cost: 0.3082160979317296\nEpoch: 90, Cost: 0.20596709468705968\n[ 0.44207137  0.76081516 -0.24090223  0.4042175   0.2024028   0.35407782\n  0.21832759  0.0165039  -0.03980088 -0.15351597  0.34953039  0.12826313\n -0.30591215 -0.13168328 -0.14439805 -0.05113714  0.1194515   0.43186841\n  0.29130757 -0.03181844 -0.24558297  0.18159045  0.14357545  0.17335863\n  0.37875114  0.28158238 -0.24018128  0.02932703 -0.28275963 -0.42715426\n -0.15118422 -0.29932214 -0.18210339  0.47112105  0.3413519   0.41564289\n  0.17038835  0.11816887  0.60066647  0.30423619 -0.33602663 -0.39995275\n  0.33467779  0.1667833   0.35331386  0.32621722  0.38033372 -0.32044388\n  0.01662089 -0.12178101]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Explanation\n1. **Preprocessing**: Clean and tokenize the text.\n2. **Building Vocabulary**: Create a vocabulary and word-to-index mapping.\n3. **Co-occurrence Matrix**: Build a co-occurrence matrix using a sliding window approach.\n4. **GloVe Model**:\n    > **Initialization**: Initialize the parameters and hyperparameters.\n    \n    > **Weighting Function**: Define a function to weight the co-occurrences.\n    \n    > **Training**: Train the model using gradient descent with AdaGrad for optimization.\n    \n    > **Get Word Vector**: Retrieve the word vector for a given word from the learned embeddings.\n    \nThis code provides a basic implementation of the GloVe model. For a more comprehensive implementation, consider using libraries like TensorFlow or PyTorch, which offer additional functionalities and optimizations.","metadata":{}}]}